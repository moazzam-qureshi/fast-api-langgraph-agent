"""
Utility functions for FastAPI and LangGraph integration
"""
from typing import Any, Dict, AsyncGenerator
import json


async def create_sse_stream(
    graph: Any,
    input_data: Dict[str, Any],
    thread_id:str
) -> AsyncGenerator[str, None]:
    """
    Stream LLM tokens as SSE using LangGraph's automatic streaming.
    
    LangGraph's stream_mode="messages" automatically captures and streams
    tokens from any LLM calls within graph nodes, even when using .invoke().
    
    Args:
        graph: LangGraph compiled graph instance
        input_data: Input dictionary for the graph
        
    Yields:
        SSE-formatted text chunks as they're generated by the LLM
    """
    config = {
        "configurable": {
            "thread_id": thread_id
        }
    }

    # LangGraph automatically streams LLM tokens with stream_mode="messages"
    async for chunk, metadata in graph.astream(input_data, stream_mode="messages", config=config):
        if getattr(chunk, "response_metadata", {}).get("model_provider") == "openai":
            yield f"data: {json.dumps({'content': chunk.content, 'id': chunk.id, 'metadata': chunk.response_metadata})}\n\n"
